---
title: GluonCV v0.5：15个新模型
author: 朱毅 Amazon Applied Scientist 金颢 Amazon Software Development Engineer
---

在用心听取用户反馈后，我们如愿在 GluonCV 0.5版带来如下的新特性:

- 新任务：视频人体行为识别
- 新模型：MobileNetV3-Large（Top-1 accuracy: 75.3）和 MobileNetV3-Small（Top-1 accuracy: 67.7）
- 新模型：更多 INT8 量化的语义分割模型
- 新模型：用于姿态估计的 AlphaPose 模型
- 新模型：用于语义分割的 VPLR 模型

本次发布囊括了15个新模型。至此，GluonCV Model Zoo中的模型总数已经增至190+个！


## [视频人体行为识别](https://gluon-cv.mxnet.io/model_zoo/action_recognition.html)

视频行为识别（Video action recognition）是识别视频中的人在进行什么动作的任务。它可以作为一个独立的应用被直接使用，比如异常行为检测，或是作为其他视频任务的基础，比如视频检索。鉴于其重要性，我们在此次发布中将视频行为识别作为一个完整的应用发布，包含模型定义、训练脚本、常用损失函数以及度量函数。为了帮助您快速启动项目，我们还提供了超越原论文精度的预训练模型以及一些教程。先来看看我们在一些油管视频上跑出来的结果。

![](img/gluon-cv-0.5-action_recognition_youtube.gif){:width="640"}

下表展示了我们提供的预训练模型在 UCF101 和 Kinetics400 数据集上的准确率。还有更强的模型（I3D, SlowFast 等）将在下一个版本登场，敬请期待。

| Model               | Pre-Trained Dataset      | Clip Length | Num of Segments |  Metric | Dataset | Accuracy |
|---------------------------|--------|-----|--------|-----|---|-----|
| vgg16_ucf101 | ImageNet  | 1 | 1 |  Top-1  |  UCF101 |  81.5  |
| vgg16_ucf101 | ImageNet  | 1 | 3 |  Top-1  | UCF101 |  83.4  |
| inceptionv3_ucf101 | ImageNet  | 1 | 1 |  Top-1  | UCF101 |  85.6  |
| inceptionv3_ucf101 | ImageNet  | 1 | 3 |  Top-1  |   UCF101 | 88.1  |
| inceptionv3_kinetics400 |   ImageNet | 1 | 3 |  Top-1   | Kinetics400 | 72.5   |


## [MobileNetV3, 再次升级](https://gluon-cv.mxnet.io/model_zoo/classification.html#mobilenet)

[MobileNetV3](https://arxiv.org/abs/1905.02244)是一个针对移动版 CPU 优化的全新神经网络结构。它是靠针对硬件性能的架构搜索和基于 NetAdapt 算法的基础上，进一步优化改进模型而得到的。MobileNetV3 相比其前一代 MobileNetV2 更为精确，并达到了两倍的提速。我们还同时提供了两份模型: MobileNetV3-Large 和 MobileNetV3-Small, 分别针对资源充裕和紧张的场景进行了优化。据我们所了解，GluonCV 是第一个提供高于论文结果的开源实现。

| Model                     | Dataset | Top-1 | Top-5 | Top-1（original paper） |
|---------------------|------|-------|-------|--------|
| MobileNetV3_Large  | ImageNet | 75.3 | 92.3 | 75.2 |
| MobileNetV3_Small  | ImageNet | 67.7 | 87.5 | 67.4 |


## [INT8量化，部署更快](https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html)

GluonCV 继续与英特尔携手带来更多的 INT8 量化模型。受益于Intel Deep Learning Boost（VNNI）的加持，用 INT8 量化后的 GluonCV 模型比它们原本的32位浮点数版本要快许多。下表是基于 AWS EC2 C5 实例的结果，在保持原有精度的情况下, 量化过的模型可以达到3到5倍的提速。不过你需要最新的 MXNet 的预览版（nightly build）才能使用这些新特性。

Model | Dataset | Batch Size | C5.12xlarge FP32 | C5.12xlarge INT8 | Speedup | FP32 Acc | INT8 Acc
-- | -- | -- | -- | -- | -- | -- | --
FCN_resnet101 | VOC | 1 | 5.46 | 26.33 | 4.82 | 97.97% | 98.00%
PSP_resnet101 | VOC | 1 | 3.96 | 10.63 | 2.68 | 98.46% | 98.45%
Deeplab_resnet101 | VOC | 1 | 4.17 | 13.35 | 3.20 | 98.36% | 98.34%
FCN_resnet101 | COCO | 1 | 5.19 | 26.22 | 5.05 | 91.28% | 90.96%
PSP_resnet101 | COCO | 1 | 3.94 | 10.60 | 2.69 | 91.82% | 91.88%
Deeplab_resnet101 | COCO | 1 | 4.15 | 13.56 | 3.27 | 91.86% | 91.98%

对于想要尝试的小伙伴, INT8 量化版本的使用和标准的 GluonCV 模型一样，仅需在模型名称后加上 `_int8` 后缀，就可以体验性能起飞的感觉！

同时, 为了方便用户在自有数据集上进行 INT8 量化，我们还提供了[量化校准工具](https://gluon-cv.mxnet.io/build/examples_deployment/int8_inference.html#calibration-tool)。目前本校准工具仅支持 Hybridized 后的 Gluon 模型，用户可以使用 `quantize_net` 接口来量化他们自己的模型, 方便各种部署。

## [用于姿态估计的AlphaPose模型](https://gluon-cv.mxnet.io/model_zoo/pose.html#alphapose)

[AlphaPose](https://arxiv.org/abs/1612.00137) 是一个由上海交通大学开发的精确多人姿态估计模型，它是第一个在 COCO 数据集上取得70+ mAP，且同时在 MPII 数据集上取得80+ mAP的实时开源模型。下表是我们复现的 AlphaPose 模型在 COCO 数据集上的表现。

| Model                     | Dataset |OKS AP | OKS AP（with flip） |
|---------------------------|---|----|-------|
| alpha_pose_resnet101_v1b_coco | COCO Keypoint  | 74.2/91.6/80.7 | 76.7/92.6/82.9 |

还有更多精确的姿态估计的例子,

![](img/gluon-cv-0.5-pose_estimation.gif){:width="640"}


## [用于语义分割的VPLR模型](https://gluon-cv.mxnet.io/model_zoo/segmentation.html#cityscapes-dataset)
我们将 [VPLR](https://arxiv.org/abs/1812.01593) 模型移植到了 GluonCV 中，这个方法在三个自动驾驶语义分割数据集 （Cityscapes, CamVid 和 KITTI）上取得了[SOTA 的结果](https://paperswithcode.com/paper/improving-semantic-segmentation-via-video)。

| Model               | Pre-Trained Dataset      | Dataset | mIoU | iIoU | 
|---------------------------|--------|-----|--------|-------|
| deeplab_v3b_plus_wideresnet_citys | ImageNet, Mapillary Vista  | Cityscapes |  83.5  | 64.4  |

下面的视频是该模型在Cityscapes数据集上的测试表现,

![](img/gluon-cv-0.5-semantic_segmentation_vplr.gif){:width="640"}

此外，本模型还有很好的泛化性能! 因为它是由更多视频帧训练得到的，而且还使用了放松边界的技巧。我们展示了一些该模型在谷歌街景图片上的预测结果, 并且和 PSPNet 进行了对比，VPLR 方法明显更加稳健。值得一提的是，这个模型是使用德国城市的图片数据训练的，而谷歌街景的图片是在美国加州拍摄的。

![](img/gluon-cv-0.5-semantic_segmentation_gsv.gif){:width="640"}

## 改进与修复

- `RCNN`模型支持了自动混合精度并整合了Horovod，在8张V100GPU上达到了接近4x的训练吞吐量。
- `RCNN`模型新增了每张GPU上多图的支持。

## 鸣谢

感谢所有在新版本中的贡献者：

@xinyu-intel (https://github.com/xinyu-intel) @hetong007 (https://github.com/hetong007) @zhreshold (https://github.com/zhreshold) @bryanyzhu (https://github.com/bryanyzhu) @Jerryzcn (https://github.com/Jerryzcn) @zhanghang1989 (https://github.com/zhanghang1989) @Laurawly (https://github.com/Laurawly) @mli (https://github.com/mli) @eric-haibin-lin (https://github.com/eric-haibin-lin) @astonzhang (https://github.com/astonzhang)@lgov (https://github.com/lgov) @zx-code123 (https://github.com/zx-code123) @Kh4L (https://github.com/Kh4L) @wuxun-zhang (https://github.com/wuxun-zhang) @mightydeveloper (https://github.com/mightydeveloper) @cygerts (https://github.com/cygerts) @feynmanliang (https://github.com/feynmanliang) @szha (http://github.com/szha) @zhouhang95 (http://github.com/zhouhang95) @yd8534976 (http://github.com/yd8534976) @wkcn (http://github.com/wkcn) @whitesockcat (http://github.com/whitesockcat) @vfdev-5 (http://github.com/vfdev-5) @mrbulb (http://github.com/mrbulb) @miraclewkf (http://github.com/miraclewkf) @hlnull (http://github.com/hlnull) @fourtunechen (http://github.com/fourtunechen) @douglas125 (http://github.com/douglas125) @algoboy101 (http://github.com/algoboy101) @Wondersui (http://github.com/Wondersui) @TakeshiKishita (https://github.com/TakeshiKishita) @SayHiRay (https://github.com/SayHiRay) @Jeff-sjtu (https://github.com/Jeff-sjtu) @HaydenFaulkner (https://github.com/HaydenFaulkner) @juliusshufan (https://github.com/juliusshufan) @ifeherva (https://github.com/ifeherva)

## 相关链接

- [GluonCV 网站](https://gluon-cv.mxnet.io/index.html)
- [GluonCV Github](https://github.com/dmlc/gluon-cv)
- [论坛讨论](https://discuss.mxnet.io/)

喜欢我们的工作并且希望支持更多的更新，欢迎点赞加星Fork！

## 参考文献

[1] Limin Wang, Yuanjun Xiong, et al. Temporal Segment Networks: Towards Good Practices for Deep Action Recognition, European Conference on Computer Vision (ECCV), 2016.

[2] Intel Deep Learning Boost. https://www.intel.ai/intel-deep-learning-boost

[3] Andrew Howard, Mark Sandler, et al. Searching for MobileNetV3, International Conference on Computer Vision (ICCV), 2019

[4] Hao-Shu Fang, Shuqin Xie, et al. RMPE: Regional Multi-person Pose Estimation, International Conference on Computer Vision (ICCV), 2017

[5] Yi Zhu, Karan Sapra, et al. Improving Semantic Segmentation via Video Propagation and Label Relaxation, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019
