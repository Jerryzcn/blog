---
title: GluonNLP v0.7.1——BERT新装上阵
author: Faramarz Munshi、林海滨 Amazon AI 翻译：金颢 Amazon Software Development Engineer
--- 

GluonNLP [7] 再次升级！我们的0.7版本提供了在大型语料库上进行了预训练的BERT Base模型，其性能可与原论文中提供的BERT Large模型媲美。其他亮点包括在不同专业领域语料库上训练后的BERT、新模型（ERNIE、GPT-2、ESIM等）以及更多数据集。完整的发布文档请猛击[这里](https://github.com/dmlc/gluon-nlp/releases)。

自从BERT进入NLP领域，整个NLP社区涌现了许多用于特定数据集和特定场景的BERT变种。从在科学领域文献和语料对BERT进行了大幅提升的SciBERT，到在生物医药文本数据上对原有BERT进行了极大改进的BioBERT [5]，各个变种在其针对的领域都体现出了它们的实用性。同时，这一波新模型的热潮又为提升BERT模型作出了巨大贡献。

以上提到的正是本次GluonNLP发布的旗舰内容。我们提供的BERT Base模型经过在共60GB的3个新语料库（OpenWebText、BooksCorpus和英文维基百科）上的预训练获得了大幅改进。由于更大语料库的加持，我们提供的Base模型的精度已经在七个任务中的六个上超越了BERT原论文中的Large模型。完整测试结果如下表所示（加粗字体代表进行比对的三个结果之中的最好结果）：

| Source    | GluonNLP                                  | BERT Base [9]                 | BERT Large [9]                |
|-----------|:-----------------------------------------:|:-----------------------------:|------------------------------:|
| Dataset   | `openwebtext_book_corpus_wiki_en_uncased` | `book_corpus_wiki_en_uncased` | `book_corpus_wiki_en_uncased` |
| SST-2     | **95.3**                                  | 93.5                          | 94.9                          |
| RTE       | **73.6**                                  | 66.4                          | 70.1                          |
| QQP       | **72.3**                                  | 71.2                          | 72.1                          |
| SQuAD 1.1 | **91.0/84.4**                             | 88.5/80.8                     | 90.9/84.1                     |
| STS-B     | **87.5**                                  | 85.8                          | 86.5                          |
| MNLI-m/mm | 85.3/84.9                                 | 84.6/83.4                     | **86.7/85.9**                 |


此外，我们也致力于简化您使用BERT完成您在具体领域上任务的步骤。加载SciBERT、BioBERT、ClinicalBERT [3]等模型仅需一行代码。同时，我们提供了一系列新模型：ERNIE [1]、GPT-2 [6]语言模型和ESIM [2]模型，都仅需一行代码即可获得。

我们还为希望在BERT模型上尝试微调的用户们提供了更多脚本，用带有丰富注释的代码填补了我们之前的一些空白：在CoNLL2003数据集上进行命名实体识别的微调脚本、中文XNLI数据集的微调脚本以及在ATIS和SNIPS数据集上进行意图识别和位置标注（slot labelling）的微调脚本。不论您希望完成的具体任务是什么，我们都为您提供了合适的工具及基本的指导，并且均可以针对您的具体自然语言处理任务进行定制。 

为了帮助您更好地测试、评估及根据需求修改我们提供的模型与脚本，我们还发布了与其匹配的数据集。在最新的发布中我们囊括了自然语言理解领域的CoLA, SST-2, MRPC, STS-B, MNLI, QQP, QNLI, WNLI, RTE数据集、情感分析领域的CR和MPQA数据集、以及用于测试意图识别与位置标注（slot labelling）的ATIS和SNIPS数据集。

还在等什么！快开始使用GluonNLP，让自然语言处理事半功倍吧！

## 开始使用GluonNLP

要开始通过GluonNLP使用BERT，请阅读涵盖了如何在多个任务上微调BERT模型的[教程](https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html?source=post_page)。此外，你也可以在[BERT Model Zoo](https://gluon-nlp.mxnet.io/model_zoo/bert/index.html)找到BERT的预训练脚本及若干数据集上的微调脚本。

欲了解GluonNLP的更多新特性，请移步我们的[发布文档](https://github.com/dmlc/gluon-nlp/releases)。同时我们也正在开发更多功能与特性，敬请期待我们下一次的发布。


## 鸣谢

在此鸣谢来自GluonNLP社区的精彩贡献：[@davisliang](https://github.com/davisliang) [@paperplanet](https://github.com/paperplanet) [@ThomasDelteil](https://github.com/ThomasDelteil) [@Deseaus](https://github.com/Deseaus) [@MarisaKirisame](https://github.com/MarisaKirisame) [@Ishitori](https://github.com/Ishitori) [@TaoLv](https://github.com/TaoLv) [@basicv8vc](https://github.com/basicv8vc) [@rongruosong](https://github.com/rongruosong) [@crcrpar](https://github.com/crcrpar) [@mrchypark](https://github.com/mrchypark) [@xwind-h](https://github.com/xwind-h) [@faramarzmunshi](https://github.com/faramarzmunshi) [@leezu](https://github.com/leezu) [@szha](https://github.com/szha) [@imgarylai](https://github.com/imgarylai) [@xiaotinghe](https://github.com/xiaotinghe) [@hankcs](https://github.com/hankcs) [@sxjscience](https://github.com/sxjscience) [@hetong007](https://github.com/hetong007) [@bikestra](https://github.com/bikestra) [@haven-jeon](https://github.com/haven-jeon) [@cgraywang](https://github.com/cgraywang) [@astonzhang](https://github.com/astonzhang) [@LindenLiu](https://github.com/LindenLiu) [@junrushao1994](https://github.com/junrushao1994)

## 参考文献

[1] Sun, Yu, et al. “ERNIE: Enhanced Representation through Knowledge Integration,” 2019;

[2] Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang: “Enhanced LSTM for Natural Language Inference”, 2016

[3] Kexin Huang, Jaan Altosaar: “ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission”, 2019

[4] Iz Beltagy, Arman Cohan: “SciBERT: Pretrained Contextualized Embeddings for Scientific Text”, 2019

[5] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So: “BioBERT: a pre-trained biomedical language representation model for biomedical text mining”, 2019

[6] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever: “Language Models are Unsupervised Multitask Learners”,” 2019

[7] Jian Guo, He He, Tong He, Leonard Lausen, Mu Li, Haibin Lin, Xingjian Shi, Chenguang Wang, Junyuan Xie, Sheng Zha, Aston Zhang, Hang Zhang, Zhi Zhang, Zhongyue Zhang: “GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural Language Processing”, 2019

[8] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” arXiv preprint arXiv:1810.04805 2018.
